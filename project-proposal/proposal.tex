\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bookmark}
\usepackage{graphicx}
\usepackage{float}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage[authoryear,sort]{natbib}
\usepackage{textcomp}
\usepackage[font=small]{caption}
\bibliographystyle{plainnat}
\addtolength{\abovecaptionskip}{-3mm}
\addtolength{\textfloatsep}{-5mm}
\setlength\columnsep{20pt}

\usepackage[a4paper,left=1.50cm, right=1.50cm, top=1.50cm, bottom=1.50cm]{geometry}


\author{Yaning You}

\title{Thesis Proposal}

\begin{document}
	
	\begin{center}
		{\Large \textbf{A Deep Learning Approach to Default Risk Migration Matrices with LLM-encoded Data}}\\
		\vspace{1em}
		{\large Yaning You}\\
		\vspace{1em}
		\textit{Department of Statistics \& Actuarial Science, University of Western Ontario}
	\end{center}
	

	\begin{center}
		\rule{150mm}{0.2mm}
	\end{center}		

	\begin{abstract}
	
	Will finish after proposal itself is fully edited
	
	\end{abstract}

	\begin{center}
		\rule{150mm}{0.2mm}
	\end{center}		

	\vspace{5mm}
	
\begin{multicols*}{2}

\section{Introduction}\label{introduction}
 
\subsection{Motivation}\label{motivation}

Credit ratings provide a relatively objective assessment of a firm's ability to repay its existing debt based off on a letter scale based on the firm's performance and external environment. This perception of reliability in turn influences lending capacity, investment instrument pricing, corporate strategy, and regulatory oversight. 
Credit ratings gained popularity and became one the most common methods of risk assessment due to its ability for quick, intuitive interpretation. However, The Great Financial Crisis of 2008 (GFC) exposed several weaknesses of the credit rating system as a investment decision making tool. The letter gradings are still too coarse to assess the credit quality of a firm outside of screening purposes.
Furthermore, credit ratings are assigned relative to other companies within each rating agency; this not only makes aggregration of firms difficult, but also does not clearly translate to the actual risk of default aside from observing historical rates experienced in each category and performing extrapolations. 

\subsection{Previous Works}\label{lit-review}

The broad and vague categories within credit ratings has been addressed as early as the late 60s with Multiple Discriminant Analysis, ranking companies in a credit portfolio along a discrete scale\citep{altman}. While Ohlson introduced a likelihood based forecasting approach, employing logistic regression on similar fundamental financial ratios as input parameter vectors \citep{ohlson}. 
Both methods produce a score relative to the data sample, reducing its applicability to broader financial data. Additionally, both methods model probability of defaults at a specific period in time, lending to poor adaptability to changes for forecasts over time periods.

\citep{shumway}'s proportional hazard duration model marked the beginning of time series based methodologies estimaing a concrete Probability of Default (PD) on a combination of accounting and market data. Using this work as a theoretical basis, \citep{duffie} introduced a double stochastic poisson process measuring "time to default" to assist in forecasting, 
overcoming the unstable yet compulsory period-by-period forecasting process for the hazard duration analysis models, while also accounting for non-default types of exits such as mergers \& acquisitions, and delistings. Unfortunately, the strict high-dimensional data requirements complicate the data formatting process, exponentiate computation time, and introduce new sources of parameter instability into the model. 
\citep{duan2012} directly improves upon the double stochastic poisson model, eliminating the previously explicit high-dimensional intermediate state variable estimation through pseudo-likelihood optimisation on CRSP firm and market information. As the function can be decomposed to observe specific parameters for different time periods, it is less computationally intensive and allows for parallel computing. 
The forward intensity estimate PDs are later used to construct the CRI-PD database used in modelling PD-implied default rates in \citep{duan2021}. This paper maps PDs from the database to the respective rating grades on the S\&P scale based on bounds optimised through data-cloning Sequential Monte Carlo (SMC) techniques \citep{duan2020} based on observed historical default occurrences and firm information from CRSP. 
This introduces a generic technique capable of mapping PD-bsed granular models back to credit ratings, which could encourage the use of PD models in industry for risk analysis and portfolio management. The model was enhanced to use credit rating migration matrices instead of historical observed default rates in \citep{duan2021-2}. Using S\&P annualized observed migration matrices from the CEREP database filtered out for non-default exits, 
the implied credit rating migration matrix is calculated for future time periods for each firm of interest. This adjustment solved a shortcoming in the previous methodology where default rates for AAA to AA+ companies had to be arbitrarily extrapolated due to lack of observed defaults, as well as reducing overestimation of companies in the AAA category. However, due to the non-continuous nature of the rating classification, 
optimisation still relies on SMC, which can degenerate and become computationally intensive \citep{bourgey}.

Introduced in \citep{li}, an alternative approach using gaussian copula functions to study default correlation between companies was developed in tandem. Unlike the first method, where the event of default is measured as a discrete probability, the random variable here is "time-until-default". The marginal distributions of firms' survival times are derived from observed market information only, which is used to determine the joint distribution of survival times for all firms.
The model was applied in \citep{bourgey} to estimate risk on a large credit portfolio and was quickly adapted in industry for calculating Collateralized Debt Obligations (CDOs) due to is ease of implementation and people's familiary with its undelying theory of joint normal distributions \citep{mackenzie}. Yet its simplicity also was the model's biggest pitfall, as its inability to account for tail dependence between parameters resulted in false impressions of CDOs being more diversified then they actually were, 
leading to the model's subsequent vilification by the public for contributing to the GFC \citep{zimmer}. \citep{allouche} sought to challenge the gaussian coupla model through implementing a temporal-based Dictionary Learning (DL) ML algorithm, constrained by autoregressive regularization. Though the implementation as small-dimension quadratic optimisation problems with linear constraints allow for significantly faster computational implementation compared to the parametric gaussian copula model,
and DL is shown to produce more accurate and consistent results compared to the gaussian copula model, the resulting rating migration matrix output consisting of relative signal strengths of rating category upgrades/downgrades is a major setback in interpretability, making the model difficult to use for pricing and regulation purposes in practice. Additionally, the model was trained on incomplete sparse data and was benchmarked against the gaussian copula model on synthetic data, casting doubt on the evident superiority
the authors claim.

As computational power improved, there have been studies implementing corporate default forecasting methods using machine learning over the aforementioned statistical methods. Studies like \citep{barboza} and \citep{sigrist} suggested improved prediction and discrimination power for decision tree based models through comparison with traditional statistical techniques. These studies show promise in applying ML methods for default forecasting capabilities; nevertheless there are several major shortcomings of these studies. 
The biggest flaw among model comparison studies like these come from the statistical benchmarks. Often the machine learning models are compared to rudimentary statistical models such as logistic regression or linear discriminant analysis; it is doubtful the improvement in performance is as significant if the benchmarks were a more complex survival analysis model. In addition, the model results may not be applicable to other markets and economies as their training data is restricted to specific countries in Europe.
Deep learning models are also explored in corporate default prediction; \citep{mai} supports the value in including textual information from 10-K filings in a CNN deep learning framework feeding document-term matrices extracted via Term-Frequency (TF) scanning. \citep{korangi} leverages transformer-based encoders to create textual data representations appropriate for a multimodal Temporal Convolution Network on mid-cap firms in the S\&P index. 

\subsection{Objectives}\label{objectives}

The approach of this study employs a similar multimodal data processing architecture to that of \citep{korangi} to forecast default probabilities in the form of credit rating migration matrices, with these following differences. Firstly, the study would be one of the first demonstrations of usaing Large Language Models (LLMs) instead of transformers to encode financial data. LLMs have shown significant potential in human-like levels of textual inference, consistently outperforming traditional pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) (\citep{tian}),
while also demonstrating decent numerical reasoning abilities with sophisticated prompting \citep{ahn}. Although there have been methods developed to encode structured data using LLMs (\citep{perozzi}), there is still a gap in current literature applying these methods using real-world data which this study hopes to begin addressing. Another part where the study is unique is incorporating graph-based Neural Networks to retain complex interactions between inputs that a transformer architecture might be too simple to capture, such as nonlinear boundary conditions in fluid particle simulations (\citep{horie}, \citep{zhang}). 
Finally, the study would like to incorporate macroeconomic indicators to investigate the interaction strength between macroeconomic performance and credit rating migration matrix probabilities. The macroeconomic and country indicators will also be of use when determining model discrimination power and forecast accuracy on training separate credit rating migration matrix forecasting models on each major index of interest (S\&P 500, STOXX 600, and TSX Composite) versus training an overarching model on all three indices together. 

\section{Methodology}\label{methodology}

The paper would break down the methodology for both the LLM encoding and temporal deep learning models, explaining the underlying theory behind each method. For the LLM multichannel data encoding, any prompt templates and prompt engineering techniques used to fine-tune the model will be disclosed, along with justification with how the application of these technqiues assist in improved encoding outputs. For the temporal graph-based neural network implementation, the paper will be structured to first provide a mathematical explanation of the central reasoning architecture, followed by a justification of the advantages of including graph knowledge structures and why this particular model was selected.
An application of the methodology will follow in a section discussing experimental design. This section will begin by introducing the datasets used throughout the study and describe the collection procedure, as well as any challenges within the dataset that required a workaround during model training. This proceeds into describing the training procedure, including the computing resources used, data preprocessing steps, the implementation of workarounds (if there were any), and the metrics used to evaluate model output. Finally, the experiment results in both computation effeciency and predictive power are revealed. This deconstruction will be repeated for standalone model training and comparison with benchmark data.
The paper will close by concluding the results of the experiment and the usefulness of the study, addressing all of the hypotheses in section ~\ref{objectives}, conceding on any shortcomings on the study, and propose areas of future investigation.

\subsection{Data Collection}\label{data}

Currently most of the data gathered for this study comes from S\&P Capital IQ. As S\&P Capital IQ is a commercial platform and access is not universal, this project will be one of the few studies utilizing this dataset and will rely on S\&P data for accuracy benchmarking. 
Data to train and evaluate the model will be collected in two steps:
\begin{enumerate}
	\item Construct credit risk data profile for constituents of S\&P 500, STOXX 600, and TSX Composite Indices from Capital IQ. 
	The composition of each credit risk data profile is as follows:
	\begin{enumerate}
		\item Fundemantal data: this is information regarding individual firms. Data will be collected in these four areas:
			- Firm volatility (This would involve calculating the \% change in the 30 day moving average from previous period)

			- Equity and Fixed income prices (30 day moving averages)

			- Financial information (financial statement items and ratios often used to assess borrowing capability)

				- Total assets

				- Loss provisions

				- Effeciency: asset turnover, cash flow cycle

				- Profitability: Return on equity, return on assets

				- Leverage: Debt to Equity, EBIT to interest expense

				- Liquidity: current ratio, tier 1 capital ratio

		\item Market data: this is information on the underlying index (so the S\&P 500, STOXX 600 and TSX Composite). Data will be collected in these areas:
		 
			- Market volatility (This would be in the form of analyzing the historical performance of the COBOE Volatility Index (VIX) and Euro STOXX 50 Volatility (VSTOXX))

			- Index prices

		\item Macroeconomic data: this is information on the country the index is derived from. Data would be collected in these areas:
	
			- real GDP change

			- Volatility (in the form of interest rates)

			- Macroeconomic trends (Considering major news events throughout the years)

	\end{enumerate} 
	\item Gather benchmarks to compare the deep learning model output with input. The model would be benchmarked on both accuracy and performance.
	For accuracy, one source of benchmarks would be the credit migration matrix results presented by the PDiR paper (insert citation here).
	The model can be trained to also return PDs of individual index constituents, which can be compared with the PDs generated by S\&P Capital IQ. 
	Currently the possibility of benchmarking the model performance with S\&P generated credit risk migration matrices is under examination; 
	if such data exists and is accessible this will provide another layer of accuracy validation. Comparison of performance will be discussed in \ref{methodology}
\end{enumerate}

\subsection{}
\begin{enumerate}
	\item Encode the data profiles using LLM into tensors to feed into deep learning model.
	Current considerations of the transforming LLM models include a Chronos-T5 embedding model to capture temporal relationships, or a General LLM (10B+ parameters) to exploit a more comprehensive knowledge base and can perhaps inference more relevant relationships among the data.
	Overall, the goal of this step is into transform the firm, equity/debt market and macroeconomic data into a format that can be understood by a deep learning model. 
	\item Generate predicted credit migration matrices using deep learning model
	
	- Graph-NN 

	- SineNet

	\item Compare model results to results produced by existing work
	
	- If permissible, test the predictive power of training one vs. multiple models 
\end{enumerate}

%\begin{itemize}
%    \item briefly describe the broad research area of the proposal;
%    \item the limitations of current research this proposal aims to address;
%    \item the motivation and impact of the work planned in this research proposal;
%    \item any additional background required for understanding the proposal.
%\end{itemize}
%
%\begin{table*}
%	\centering
%	\begin{tabular}{cc}
%		\hline
%		\textbf{Citation format} & \textbf{Citation command} \\
%		\hline
%		\citet{APA:83} & \textbackslash{}citet{} \\
%		\citep{APA:83} & \textbackslash{}citep{} \\
%		\hline
%	\end{tabular}
%	\caption{This is sample table with full page width.}
%	\label{tbl:tbl1}
%\end{table*}
%
%	
%\begin{figure}[H]
%    \centering
%	\includegraphics[width=\columnwidth]{example-image}
%	\caption{This is a sample figure.}
%	\label{fig:fig1}
%\end{figure}

\end{multicols*}

\clearpage

\bibliography{bb-ds-fellowship}
	
\end{document}