\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bookmark}
\usepackage{graphicx}
\usepackage{float}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{textcomp}
\usepackage[font=small]{caption}
\bibliographystyle{plainnat}
\addtolength{\abovecaptionskip}{-3mm}
\addtolength{\textfloatsep}{-5mm}
\setlength\columnsep{20pt}

\usepackage[a4paper,left=1.50cm, right=1.50cm, top=1.50cm, bottom=1.50cm]{geometry}


\author{Yaning You}

\title{Thesis Proposal}

\begin{document}
	
	\begin{center}
		{\Large \textbf{A Deep Learning Approach to Default Risk Migration Matrices with LLM-encoded Data}}\\
		\vspace{1em}
		{\large Yaning You}\\
		\vspace{1em}
		\textit{Department of Statistics \& Actuarial Science, University of Western Ontario}
	\end{center}
	

	\begin{center}
		\rule{150mm}{0.2mm}
	\end{center}		

	\begin{abstract}
	
	Will finish after proposal itself is fully edited
	
	\end{abstract}

	\begin{center}
		\rule{150mm}{0.2mm}
	\end{center}		

	\vspace{5mm}
	
\begin{multicols*}{2}

\section{Introduction}\label{introduction}

Interest in capital adequacy assessments have grown exponentially since the signing of the Basel II accord in 2004, shortly after the dot com bubble burst where investment behavior was less than prudent.
As time progressed and the world struggled through the Great Financial Crisis of 2008, interest in accurate default forecasts to assess financial capability only grew. As an central component of assessing obligor credit quality,
pricing credit related financial instruments such as bonds and credit default swaps, and measuring impact of regulation, default forecast methodologies that are accurate, explainable and robust to the dynamic financial landscape can significantly benefit the operational consistency of financial systems across the world.  
 
\subsection{Motivation}\label{motivation}

Corpoate defaults impact the financial system beyond the obligor itself. It represents an immediate financial loss in the company's investors, lenders, and depending on the number of defaults in a time period, the economic loss to the country the company is based in.
Understanding the likelihood of a company defaulting helps creditors screen for potential borrowers for more secure returns, investors price instruments at fair value and manage portfolios, and regulators assess the state of the market and decide whether or not to intervene.
Thus models of assessing default risk were created by these parties to quantify the risk and track it over time, one of the most popular models being credit ratings.  

Credit ratings provide a relatively objective assessment of a firm's ability to repay its existing debt based off of capital structure, historical performance, internal synergies and external influences on a letter scale.
This perception of reliability in turn influences lending capacity, investment instrument pricing, corporate strategy, and regulatory oversight of the firm as previously mentioned. 
Credit ratings gained popularity and became one the most common methods of risk assessment due to two factors. Firstly, it is constructed mainly from firm's financial and economic fundmanetal characteristics, lending the ratings to be highly explicable.
More importantly, the ratings are arranged into a letter scale from the highest quality of AAA to companies already in default at D, allowing quick interpretation and decision making, providing a boon for portfolio managers who often face pressure to make high-impact decisions on a tight schedule.
Naturally it has been adopted by many in risk modelling and investment management.

The Great Financial Crisis of 2008 (GFC) exposed several weaknesses of the credit rating system as a investment decision making tool. The letter gradings are still too coarse to assess the credit quality of a firm outside of screening purposes.
Furthermore, credit ratings are assigned relative to other companies within each rating agency; this not only makes aggregration of firms difficult, but also does not clearly translate to the actual risk of default aside from observing historical rates experienced in each category and performing extrapolations.
This means credit ratings are a lagging indicator and are slow to adapt changes in the general state of the economy, particularly changes in the broad environment that may not be directly correlated with the firm, giving the rise in demand for a more granular and objective measure of default risk and the birth of the Probability of Default model, henceforth referred to as PD.
    
\subsubsection{Probability of Default Models in Industry}\label{industry}

Unlike credit ratings which assign firms to a finite, often restrictive, set of credit quality indicators, a PD estimates a concrete probability to the firm experiencing financial default.
This allows rigorous comparisons between firms across agencies as well as scientific examinations of risk management practices. Due to these factors, PD has quickly gained popularity of usage in credit risk modelling, loan pricing, and portfolio management.
There are many ways to model PDs, ranging from a simple logistic regression on observed historical defaults to complex multistep models using firm, market and country-level performance over 10+ years. 
As of the present, all of the "big three" (S\&P, Moody's and Fitch) ratings agencies, as well as many financial data service providers (eg. Bloomberg, Morningstar Refinitiv) are offering PD estimates for firms in its database, a testament to the popularity of PDs in commercial use.

The PD models for each of the three firms is undisclosed; yet there are interesting numances from what information they have revealed. 
The S\&P model uses a dataset of "default flags" (ie. financial statement ratios that can signal default) and derives its PDs with a Maximal Utility approach for optimisation and k-fold Greedy Forward for variable selection bounded by AIC for model parsimony \citep{SP-PD-methodology}. 
Fitch employs a Monte Carlo simulation model to estimate the probability and frequency of a firm's assets falling below its liabilities, mostly relying on historical credit rating data \citep{Fitch-PD-methodology}. 
Moody's exhibits the most comprehensive and complex model, using macroeconomic indicators on top of individual firm information, as well as incorporating credit migration matrices to calibrate its model over longer periods of time. The PDs appear to be calculated through a censorship-corrected survival analysis model \citep{Moodys-PD-methodology}.

Overall, these models use similar data as the credit ratings and employ relatively straightforward methodologies to model the occurrence of defaults within a credit portfolio. This once again lends to highly explicable PD results that might not fully capture the complexities in the market and broad economy.
Naturally, more complex methods of modelling PDs emerged. The proposal will introduce the most straightforward, and once again most popular method, the credit rating migration matrix, in the next subsection, and will provide an overview of  \ref{lit-review}.

\subsubsection{Credit Rating Migration Matrix}\label{crmm}

In a portfolio, companies exist in more states than simply default and non-default. The credit rating migration matrix captures a portion of this information by also calculating the probabilities a certain firm improves or degrades in percieved credit quality. 
Often the rating migrations are mapped to existing credit rating categories for ease of interpretation. The probabilities of migrating into other categories provide an additional level of granularity into corporate default prediction, providing more stable forecasts into future default occurrences, 
as well as providing stress testing capabilities on existing portfolios. As such the ability to generate accurate, dynamic credit rating migration matrices is crucial to effective pricing and risk management across the financial services industry, motivating the development of credit rating migration matrix forecasting methodologies which will be outlined in \ref{lit-review} and \ref{objectives}.

\subsection{Previous Works}\label{lit-review}

The coarseness associated with credit ratings has been addressed as early as the late 60s with Altam's Z score. The function takes multiple solvency ratios, computes a score relative to the firm group using the MDA technique, and firms are ranked using the MDA scores to determine credit risk \citep{altman}. Despite being simple to apply and provides more granularity than basic credit ratings, 
the resulting scores only represent a ranking between the firms and have little intuitive interpretation value otherwise. To counteract this issue, Ohlson introduced a likelihood based forecasting approach, employing logistic regression on similar fundamental financial ratios as input parameter vectors \citep{ohlson}. This significantly improves upon MDA based methods by producing a score that directly ties with 
the probability of default, and a correlation matrix between the input components and probability of default can be generated to analyze for significant parameters. Nevertheless, both methods ultimately only produce a score relative to the data sample, reducing its applicability to broader financial data. Additionally, both methods model probability of defaults at a specific period in time, lending to poor adaptability
to changes for forecasts over singular time periods.

Time series-based models arose as a response to this particular issue. Two major approaches of temporal-based analysis arose. Starting with \citep{shumway} on a reduced set of market-driven (as opposed to the traditional accounting driven) variables, the first approach involve proportional hazard duration analysis and favored the use of market/index parameters in prediction (\citep{chava}, \citep{hillegeist}, \citep{hull}).
Using this work as a theoretical basis, \citep{duffie} introduced a double stochastic poisson process measuring "time to default" to assist in forecasting, with several advantages. It achieves the main objective of overcoming the compulsory period-by-period forecasting process for the hazard duration analysis models, but also accounts for non-default types of exits such as mergers \& acquisitions, and delistings. 
Unfortunately, the strict high-dimensional data requirements complicate the data formatting process, exponentiate computation time, and introduce parameter instability into the model. 
\citep{duan2012} directly improves upon the double stochastic poisson model, eliminating the previously explicit high-dimensional intermediate state variable estimation through pseudo-likelihood optimisation on CRSP firm and market information. As the function can be decomposed to observe specific parameters for different time periods, it is less computationally intensive and allows for parallel computing. 
The forward intensity estimate PDs are later used to construct the CRI-PD database used in modelling PD-implied default rates in \citep{duan2021}. This paper maps PDs from the database to the respective rating grades on the S\&P scale based on bounds optimised through data-cloning Sequential Monte Carlo (SMC) techniques \citep{duan2020} based on observed historical default occurrences and firm information from CRSP. 
This introduces a generic technique capable of mapping PD-bsed granular models back to credit ratings, which could encourage the use of PD models in industry for risk analysis and portfolio management. The model was enhanced to use credit rating migration matrices instead of historical observed default rates in \citep{duan2021-2}. Using S\&P annualized observed migration matrices from the CEREP database filtered out for non-default exits, 
the implied credit rating migration matrix is calculated for future time periods for each firm of interest. This adjustment solved a shortcoming in the previous methodology where default rates for AAA to AA+ companies had to be arbitrarily extrapolated due to lack of observed defaults, as well as reducing overestimation of companies in the AAA category. However, due to the non-continuous nature of the rating classification, 
optimisation still relies on SMC, which can degenerate and become computationally intensive \citep{bourgey}.

Introduced in \citep{li}, the second approach uses gaussian copula functions to study default correlation between companies. Unlike the first method, where the event of default is measured as a discrete probability, the random variable here is "time-until-default". The marginal distributions of firms' survival times are derived from observed market information only, which is used to determine the joint distribution of survival times for all firms.
The model was applied in \citep{bourgey} to estimate risk on a large credit portfolio and was quickly adapted in industry for calculating Collateralized Debt Obligations (CDOs) due to is ease of implementation and people's familiary with its undelying theory of joint normal distributions \citep{mackenzie}. Yet its simplicity also was the model's biggest pitfall, as its inability to account for tail dependence between parameters resulted in false impressions of CDOs being more diversified then they actually were, 
leading to the model's subsequent vilification by the public for contributing to the GFC \citep{zimmer}. \citep{allouche} sought to challenge the gaussian coupla model through implementing a temporal-based Dictionary Learning (DL) ML algorithm, constrained by autoregressive regularization. Though the implementation as small-dimension quadratic optimisation problems with linear constraints allow for significantly faster computational implementation compared to the parametric gaussian copula model,
and DL is shown to produce more accurate and consistent results compared to the gaussian copula model, the resulting rating migration matrix output consisting of relative signal strengths of rating category upgrades/downgrades is a major setback in interpretability, making the model difficult to use for pricing and regulation purposes in practice. Additionally, the model was trained on incomplete sparse data and was benchmarked against the gaussian copula model on synthetic data, casting doubt on the evident superiority
the authors claim.

As computational power improved, there have been studies comparing the performance between ML models and traditional ststistical methods. Studies like \citep{barboza} and \citep{sigrist} suggested improved prediction and discrimination power for decision tree based models through comparison with traditional statistical techniques. These studies show promise in applying ML methods for default forecasting capabilities; nevertheless there are several major shortcomings of these studies. 
The biggest flaw among model comparison studies like these come from the statistical benchmarks. Often the machine learning models are compared to rudimentary statistical models such as logistic regression or linear discriminant analysis; it is doubtful the improvement in performance is as significant if the benchmarks were a more complex survival analysis model. In addition, the model results may not be applicable to other markets and economies as their training data is restricted to specific countries in Europe.
Deep learning models are also explored in corporate default prediction; \citep{mai} supports the value in including textual information from 10-K filings in a CNN deep learning framework feeding document-term matrices extracted via Term-Frequency (TF) scanning. \citep{korangi} leverages transformer-based encoders to create textual data representations appropriate for a multimodal Temporal Convolution Network on mid-cap firms in the S\&P index. 

\subsection{Objectives}\label{objectives}

The approach of this study employs a similar multimodal data processing architecture to that of \citep{korangi} to forecast default probabilities in the form of credit rating migration matrices, with these following differences. Firstly, the study would be one of the first demonstrations of usaing Large Language Models (LLMs) instead of transformers to encode financial data. LLMs have shown significant potential in human-like levels of textual inference, consistently outperforming traditional pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) (\citep{tian}),
while also demonstrating decent numerical reasoning abilities with sophisticated prompting \citep{ahn}. Although there have been methods developed to encode structured data using LLMs (\citep{perozzi}), there is still a gap in current literature applying these methods using real-world data which this study hopes to begin addressing. Another part where the study is unique is incorporating graph-based Neural Networks to retain complex interactions between inputs that a transformer architecture might be too simple to capture, such as nonlinear boundary conditions in fluid particle simulations (\citep{horie}, \citep{zhang}). 
Finally, the study would like to incorporate macroeconomic indicators to investigate the interaction strength between macroeconomic performance and credit rating migration matrix probabilities. The macroeconomic and country indicators will also be of use when determining model discrimination power and forecast accuracy on training separate credit rating migration matrix forecasting models on each major index of interest (S\&P 500, STOXX 600, and TSX Composite) versus training an overarching model on all three indices together. 

\section{Methodology}\label{methodology}

The paper would break down the methodology for both the LLM encoding and temporal deep learning models, explaining the underlying theory behind each method. For the LLM multichannel data encoding, any prompt templates and prompt engineering techniques used to fine-tune the model will be disclosed, along with justification with how the application of these technqiues assist in improved encoding outputs. For the temporal graph-based neural network implementation, the paper will be structured to first provide a mathematical explanation of the central reasoning architecture, followed by a justification of the advantages of including graph knowledge structures and why this particular model was selected.
An application of the methodology will follow in a section discussing experimental design. This section will begin by introducing the datasets used throughout the study and describe the collection procedure, as well as any challenges within the dataset that required a workaround during model training. This proceeds into describing the training procedure, including the computing resources used, data preprocessing steps, the implementation of workarounds (if there were any), and the metrics used to evaluate model output. Finally, the experiment results in both computation effeciency and predictive power are revealed. This deconstruction will be repeated for standalone model training and comparison with benchmark data.
The paper will close by concluding the results of the experiment and the usefulness of the study, addressing all of the hypotheses in section ~\ref{objectives}, conceding on any shortcomings on the study, and propose areas of future investigation.

\subsection{Data Collection}\label{data}

Currently most of the data gathered for this study comes from S\&P Capital IQ. As S\&P Capital IQ is a commercial platform and access is not universal, this project will be one of the few studies utilizing this dataset and will rely on S\&P data for accuracy benchmarking. 
Data to train and evaluate the model will be collected in two steps:
\begin{enumerate}
	\item Construct credit risk data profile for constituents of S\&P 500, STOXX 600, and TSX Composite Indices from Capital IQ. 
	The composition of each credit risk data profile is as follows:
	\begin{enumerate}
		\item Fundemantal data: this is information regarding individual firms. Data will be collected in these four areas:
			- Firm volatility (This would involve calculating the \% change in the 30 day moving average from previous period)

			- Equity and Fixed income prices (30 day moving averages)

			- Financial information (financial statement items and ratios often used to assess borrowing capability)

				- Total assets

				- Loss provisions

				- Effeciency: asset turnover, cash flow cycle

				- Profitability: Return on equity, return on assets

				- Leverage: Debt to Equity, EBIT to interest expense

				- Liquidity: current ratio, tier 1 capital ratio

		\item Market data: this is information on the underlying index (so the S\&P 500, STOXX 600 and TSX Composite). Data will be collected in these areas:
		 
			- Market volatility (This would be in the form of analyzing the historical performance of the COBOE Volatility Index (VIX) and Euro STOXX 50 Volatility (VSTOXX))

			- Index prices

		\item Macroeconomic data: this is information on the country the index is derived from. Data would be collected in these areas:
	
			- real GDP change

			- Volatility (in the form of interest rates)

			- Macroeconomic trends (Considering major news events throughout the years)

	\end{enumerate} 
	\item Gather benchmarks to compare the deep learning model output with input. The model would be benchmarked on both accuracy and performance.
	For accuracy, one source of benchmarks would be the credit migration matrix results presented by the PDiR paper (insert citation here).
	The model can be trained to also return PDs of individual index constituents, which can be compared with the PDs generated by S\&P Capital IQ. 
	Currently the possibility of benchmarking the model performance with S\&P generated credit risk migration matrices is under examination; 
	if such data exists and is accessible this will provide another layer of accuracy validation. Comparison of performance will be discussed in \ref{methodology}
\end{enumerate}

\subsection{}
\begin{enumerate}
	\item Encode the data profiles using LLM into tensors to feed into deep learning model.
	Current considerations of the transforming LLM models include a Chronos-T5 embedding model to capture temporal relationships, or a General LLM (10B+ parameters) to exploit a more comprehensive knowledge base and can perhaps inference more relevant relationships among the data.
	Overall, the goal of this step is into transform the firm, equity/debt market and macroeconomic data into a format that can be understood by a deep learning model. 
	\item Generate predicted credit migration matrices using deep learning model
	
	- Graph-NN 

	- SineNet

	\item Compare model results to results produced by existing work
	
	- If permissible, test the predictive power of training one vs. multiple models 
\end{enumerate}

%\begin{itemize}
%    \item briefly describe the broad research area of the proposal;
%    \item the limitations of current research this proposal aims to address;
%    \item the motivation and impact of the work planned in this research proposal;
%    \item any additional background required for understanding the proposal.
%\end{itemize}
%
%\begin{table*}
%	\centering
%	\begin{tabular}{cc}
%		\hline
%		\textbf{Citation format} & \textbf{Citation command} \\
%		\hline
%		\citet{APA:83} & \textbackslash{}citet{} \\
%		\citep{APA:83} & \textbackslash{}citep{} \\
%		\hline
%	\end{tabular}
%	\caption{This is sample table with full page width.}
%	\label{tbl:tbl1}
%\end{table*}
%
%	
%\begin{figure}[H]
%    \centering
%	\includegraphics[width=\columnwidth]{example-image}
%	\caption{This is a sample figure.}
%	\label{fig:fig1}
%\end{figure}

\end{multicols*}

\clearpage

\bibliography{bb-ds-fellowship}
	
\end{document}