@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@misc{SP-PD,
    author    = {S\&P Global},
    title     = {Probability of Default and Scoring Models: Similarities and Differences},
    year      = "2021",
    url       = "https://www.spglobal.com/market-intelligence/en/news-insights/research/probability-of-default-and-scoring-models-similarities-and-differences"
}

@misc{SP-PD-methodology,
  author = {Chen, A. and Baldassarri, G.},
  title = {PD MODEL FUNDAMENTALS: BANKS - A Pioneer Model for Assessing Bank Creditworthiness},
  year = {2015},
  url = {https://www.spglobal.com/marketintelligence/en/documents/pd-model-fundamentals-banks-a-pioneer-model-for-assessing-bank-creditworthiness.pdf}
}

@misc{Fitch-PD-methodology,
  author = {Fitch Ratings},
  title = {Fitch Portfolio Credit Model},
  year = {2013},
  url = {https://www.fitchratings.com/fitch-portfolio-credit-model},
}

@misc{Moodys-PD-methodology,
  author = {Moody's},
  title = {Features of a lifetime PD model: Evidence from public, private, and rated firms},
  year = {2018},
  url = {https://www.moodys.com/web/en/us/insights/credit-risk/features-of-a-lifetime-pd-model.html},
}

@article{altman,
 ISSN = {00221082, 15406261},
 URL = {http://www.jstor.org/stable/2978933},
 author = {Edward I. Altman},
 journal = {The Journal of Finance},
 number = {4},
 pages = {589--609},
 publisher = {[American Finance Association, Wiley]},
 title = {Financial Ratios, Discriminant Analysis and the Prediction of Corporate Bankruptcy},
 urldate = {2024-10-10},
 volume = {23},
 year = {1968}
}

@article{ohlson,
 ISSN = {00218456, 1475679X},
 URL = {http://www.jstor.org/stable/2490395},
 author = {James A. Ohlson},
 journal = {Journal of Accounting Research},
 number = {1},
 pages = {109--131},
 publisher = {[Accounting Research Center, Booth School of Business, University of Chicago, Wiley]},
 title = {Financial Ratios and the Probabilistic Prediction of Bankruptcy},
 urldate = {2024-10-10},
 volume = {18},
 year = {1980}
}

@article{shumway,
Abstract = {This article describes a hazard model that can be used for forecasting bankruptcy. Static models are inappropriate for forecasting bankruptcy because of the nature of bankruptcy data. Since bankruptcy occurs infrequently, forecasters use samples that span several years to estimate their models. The characteristics of most firms change from year to year. However, static models can only consider one set of explanatory variables for each firm. Researchers who apply static models to bankruptcy have to select when to observe each firm's characteristics. Most forecasters choose to observe each bankrupt firm's data in the year before bankruptcy. They ignore data on healthy firms that eventually go bankrupt. By choosing when to observe each firm's characteristics arbitrarily, forecasters who use static models introduce an unnecessary selection bias into their estimates. Hazard models resolve the problems of static models by explicitly accounting for time. The dependent variable in a hazard mo},
Author = {Shumway, Tyler},
ISSN = {00219398},
Journal = {Journal of Business},
Keywords = {Bankruptcy, Business forecasting, Accounting, Accounting problems &amp; exercises, Models &amp; modelmaking, Nature},
Number = {1},
Pages = {101},
Title = {Forecasting Bankruptcy More Accurately: A Simple Hazard Model.},
Volume = {74},
URL = {https://search.ebscohost.com/login.aspx?direct=true&amp;db=bth&amp;AN=4043236&amp;site=ehost-live},
Year = {2001},
}

@article{chava,
    author = {Chava, Sudheer and Jarrow, Robert A.},
    title = "{Bankruptcy Prediction with Industry Effects*}",
    journal = {Review of Finance},
    volume = {8},
    number = {4},
    pages = {537-569},
    year = {2004},
    month = {01},
    abstract = "{This paper investigates the forecasting accuracy of bankruptcy hazard rate models for U.S. companies over the time period 1962–1999 using both yearly and monthly observation intervals. The contribution of this paper is multiple-fold. One, using an expanded bankruptcy database we validate the superior forecasting performance of Shumway's (2001) model as opposed to Altman (1968) and Zmijewski (1984). Two, we demonstrate the importance of including industry effects in hazard rate estimation. Industry groupings are shown to significantly affect both the intercept and slope coefficients in the forecasting equations. Three, we extend the hazard rate model to apply to financial firms and monthly observation intervals. Due to data limitations, most of the existing literature employs only yearly observations. We show that bankruptcy prediction is markedly improved using monthly observation intervals. Fourth, consistent with the notion of market efficiency with respect to publicly available information, we demonstrate that accounting variables add little predictive power when market variables are already included in the bankruptcy model.}",
    issn = {1572-3097},
    doi = {10.1093/rof/8.4.537},
    url = {https://doi.org/10.1093/rof/8.4.537},
    eprint = {https://academic.oup.com/rof/article-pdf/8/4/537/26322110/8-4-537.pdf},
}

@article{hillegeist,
  author = {Hillegeist, Stephen A. et al.},
  journal = {Review of Accounting Studies},
  number = {1},
  title = {Assessing the Probability of Bankruptcy},
  volume = {9},
  year = {2004},
  month = {3},
  issn = {1573-7136},
  doi = {10.1023/B:RAST.0000013627.90884.b7},
  url = {https://doi.org/10.1023/B:RAST.0000013627.90884.b7},
}

@article{duffie,
title = {Multi-period corporate default prediction with stochastic covariates},
journal = {Journal of Financial Economics},
volume = {83},
number = {3},
pages = {635-665},
year = {2007},
issn = {0304-405X},
doi = {https://doi.org/10.1016/j.jfineco.2005.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0304405X06002029},
author = {Darrell Duffie and Leandro Saita and Ke Wang},
keywords = {Default, Bankruptcy, Duration analysis, Doubly stochastic, Distance to default},
abstract = {We provide maximum likelihood estimators of term structures of conditional probabilities of corporate default, incorporating the dynamics of firm-specific and macroeconomic covariates. For US Industrial firms, based on over 390,000 firm-months of data spanning 1980 to 2004, the term structure of conditional future default probabilities depends on a firm's distance to default (a volatility-adjusted measure of leverage), on the firm's trailing stock return, on trailing S&P 500 returns, and on US interest rates. The out-of-sample predictive performance of the model is an improvement over that of other available models.}
}

@techreport{hull,
address = {Toronto},
author = {Hull, John and White, Alan},
file = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1295225#paper-citations-widget},
institution = {Joseph L. Rotman School of Management, University of Toronto},
series = {Valuing Credit Default Swaps},
title = {{Valuing Credit Default Swaps II: Modelling Default Correlations}},
year = {2000}
}

@article{duan2012,
title = {Multiperiod corporate default prediction—A forward intensity approach},
journal = {Journal of Econometrics},
volume = {170},
number = {1},
pages = {191-209},
year = {2012},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2012.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0304407612001145},
author = {Jin-Chuan Duan and Jie Sun and Tao Wang},
keywords = {Default, Bankruptcy, Forward intensity, Maximum pseudo-likelihood, Forward default probability, Cumulative default probability, Accuracy ratio},
abstract = {A forward intensity model for the prediction of corporate defaults over different future periods is proposed. Maximum pseudo-likelihood analysis is then conducted on a large sample of the US industrial and financial firms spanning the period 1991–2011 on a monthly basis. Several commonly used factors and firm-specific attributes are shown to be useful for prediction at both short and long horizons. Our implementation also factors in momentum in some variables and documents their importance in default prediction. The model’s prediction is very accurate for shorter horizons. Its accuracy deteriorates somewhat when the horizon is increased to two or three years, but the performance still remains reasonable. The forward intensity model is also amenable to aggregation, which allows for an analysis of default behavior at the portfolio and/or economy level.}
}

@Inbook{duan2021,
author="Duan, Jin-Chuan
and Li, Shuping",
title="PD-Implied Ratings via Referencing a Credit Rating/Scoring Pool's Default Experience",
bookTitle="Behavioral Predictive Modeling in Economics",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="105--115",
abstract="Letter-based credit ratings are deeply rooted in commercial and regulatory practices, which in a way impedes a wider and quicker adoption of scientifically rigorous and operationally superior and granular credit risk measures such as probability of default (PD). This paper reverse-engineers a mapping methodology converting PDs to letter ratings by referencing the realized default rates of different rating categories experienced by a commercial rating agency such as standard {\&} Poor's or Moody's.",
isbn="978-3-030-49728-6",
doi="10.1007/978-3-030-49728-6_6",
url="https://doi.org/10.1007/978-3-030-49728-6_6"
}

@article{duan2020,
title = {Data-cloning SMC2: A global optimizer for maximum likelihood estimation of latent variable models},
journal = {Computational Statistics \& Data Analysis},
volume = {143},
pages = {106841},
year = {2020},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2019.106841},
url = {https://www.sciencedirect.com/science/article/pii/S0167947319301963},
author = {Jin-Chuan Duan and Andras Fulop and Yu-Wei Hsieh},
keywords = {Sequential Monte Carlo, Data clone, Latent variable, Maximum likelihood, Monte Carlo optimization},
abstract = {A data-cloning SMC2 algorithm is proposed as a general-purpose, global optimization routine for the maximum likelihood estimation of models with latent variables. In the SMC2 phase, the method first marginalizes out the latent variable(s) by applying one layer of SMC at a fixed parameter value and then searches for the optimal parameters through another layer of SMC. The data-cloning phase is deployed to ensure global convergence by dampening multi-modality and to reduce the Monte Carlo error associated with SMC. This new method has broad applicability and is massively parallelizable through leveraging modern multi-core CPU or GPU computing.}
}

@article{duan2021-2,
title = {Enhanced PD-implied ratings by targeting the credit rating migration matrix},
journal = {The Journal of Finance and Data Science},
volume = {7},
pages = {115-125},
year = {2021},
issn = {2405-9188},
doi = {https://doi.org/10.1016/j.jfds.2021.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2405918821000052},
author = {Jin-Chuan Duan and Shuping Li},
keywords = {Default, Other-exit, Rating stickiness, Sequential Monte Carlo},
abstract = {A high-quality and granular probability of default (PD) model is on many practical dimensions far superior to any categorical credit rating system. Business adoption of a PD model, however, needs to factor in the long-established business/regulatory conventions built around letter-based credit ratings. A mapping methodology that converts granular PDs into letter ratings via referencing the historical default experience of some credit rating agency exists in the literature. This paper improves the PD implied rating (PDiR) methodology by targeting the historical credit migration matrix instead of simply default rates. This enhanced PDiR methodology makes it possible to bypass the reliance on arbitrarily extrapolated target default rates for the AAA and AA+ categories, a necessity due to the fact that the historical realized default rates on these two top rating grades are typically zero.}
}

@article{bourgey,
author = {Bourgey, Florian and Gobet, Emmanuel and Rey, Cl\'{e}ment},
title = {Metamodel of a Large Credit Risk Portfolio in the Gaussian Copula Model},
journal = {SIAM Journal on Financial Mathematics},
volume = {11},
number = {4},
pages = {1098-1136},
year = {2020},
doi = {10.1137/19M1292084},
URL = {https://doi.org/10.1137/19M1292084},
eprint = {https://doi.org/10.1137/19M1292084},
abstract = { We design a metamodel for the loss distribution \${\mathcal L}\$ of a large credit risk portfolio in the Gaussian copula model. Our procedure is twofold. We first apply the Wiener chaos decomposition on the normal systemic economic factor and derive a truncated loss \${\mathcal{L}\_{I}}\$ at some order \$I\$. Then, we provide a Gaussian approximation \${\mathcal{L}\_{I}^{{G}}}\$ of the associated truncated loss. Such an approach is motivated by the fact that we are dealing with large portfolios. Our procedure significantly reduces the computational time needed for sampling the loss and therefore for estimating risk measures. The accuracy and effectiveness of our method are confirmed by numerical examples. }
}

@article{li,
  title={On default correlation: A copula function approach},
  author={Li, David X},
  journal={Available at SSRN 187289},
  year={1999},
  doi = {https://dx.doi.org/10.2139/ssrn.187289},
  url = {https://ssrn.com/abstract=187289},
}

@article{mackenzie,
 ISSN = {03063127},
 URL = {http://www.jstor.org/stable/43284238},
 abstract = {Drawing on documentary sources and 114 interviews with market participants, this and a companion article discuss the development and use in finance of the Gaussian copula family of models, which are employed to estimate the probability distribution of losses on a pool of loans or bonds, and which were centrally involved in the credit crisis. This article, which explores how and why the Gaussian copula family developed in the way it did, employs the concept of 'evaluation culture', a set of practices, preferences and beliefs concerning how to determine the economic value of financial instruments that is shared by members of multiple organizations. We identify an evaluation culture, dominant within the derivatives departments of investment banks, which we call the 'culture of no-arbitrage modelling', and explore its relation to the development of Gaussian copula models. The article suggests that two themes from the science and technology studies literature on models (modelling as 'impure' bricolage, and modelling as articulating with heterogeneous objectives and constraints) help elucidate the history of Gaussian copula models in finance.},
 author = {Donald MacKenzie and Taylor Spears},
 journal = {Social Studies of Science},
 number = {3},
 pages = {393--417},
 publisher = {Sage Publications, Ltd.},
 title = {'The formula that killed Wall Street': The Gaussian copula and modelling practices in investment banking},
 urldate = {2024-10-11},
 volume = {44},
 year = {2014}
}

@article{zimmer,
 ISSN = {00346535, 15309142},
 URL = {http://www.jstor.org/stable/23262091},
 abstract = {Due to its simplicity and familiarity, the Gaussian copula is popular in calculating risk in collaterized debt obligations, but it imposes asymptotic independence such that extreme events appear to be unrelated. This restriction might be innocuous in normal times, but during extreme events, such as the housing crisis, the Gaussian copula might be inappropriate. This paper explores various copula specifications and finds that the degree to which housing prices are related based on the Gaussian copula is too small compared with real housing price data.},
 author = {David M. Zimmer},
 journal = {The Review of Economics and Statistics},
 number = {2},
 pages = {607--620},
 publisher = {The MIT Press},
 title = {THE ROLE OF COPULAS IN THE HOUSING CRISIS},
 urldate = {2024-10-11},
 volume = {94},
 year = {2012}
}

@article{allouche,
author = {Allouche, Michael and Gobet, Emmanuel and Lage, Clara and Mangin, Edwin},
year = {2024},
month = {01},
pages = {3431-3456},
title = {Structured dictionary learning of rating migration matrices for credit risk modeling},
volume = {39},
journal = {Computational Statistics},
doi = {10.1007/s00180-023-01449-y},
url = {https://link.springer.com/article/10.1007/s00180-023-01449-y#citeas},
}

@article{barboza,
title = {Machine learning models and bankruptcy prediction},
journal = {Expert Systems with Applications},
volume = {83},
pages = {405-417},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417302415},
author = {Flavio Barboza and Herbert Kimura and Edward Altman},
keywords = {Bankruptcy prediction, Machine learning, Support vector machines, Boosting, Bagging, Random forest},
abstract = {There has been intensive research from academics and practitioners regarding models for predicting bankruptcy and default events, for credit risk management. Seminal academic research has evaluated bankruptcy using traditional statistics techniques (e.g. discriminant analysis and logistic regression) and early artificial intelligence models (e.g. artificial neural networks). In this study, we test machine learning models (support vector machines, bagging, boosting, and random forest) to predict bankruptcy one year prior to the event, and compare their performance with results from discriminant analysis, logistic regression, and neural networks. We use data from 1985 to 2013 on North American firms, integrating information from the Salomon Center database and Compustat, analysing more than 10,000 firm-year observations. The key insight of the study is a substantial improvement in prediction accuracy using machine learning techniques especially when, in addition to the original Altman’s Z-score variables, we include six complementary financial indicators. Based on Carton and Hofer (2006), we use new variables, such as the operating margin, change in return-on-equity, change in price-to-book, and growth measures related to assets, sales, and number of employees, as predictive variables. Machine learning models show, on average, approximately 10% more accuracy in relation to traditional models. Comparing the best models, with all predictive variables, the machine learning technique related to random forest led to 87% accuracy, whereas logistic regression and linear discriminant analysis led to 69% and 50% accuracy, respectively, in the testing sample. We find that bagging, boosting, and random forest models outperform the others techniques, and that all prediction accuracy in the testing sample improves when the additional variables are included. Our research adds to the discussion of the continuing debate about superiority of computational methods over statistical techniques such as in Tsai, Hsu, and Yen (2014) and Yeh, Chi, and Lin (2014). In particular, for machine learning mechanisms, we do not find SVM to lead to higher accuracy rates than other models. This result contradicts outcomes from Danenas and Garsva (2015) and Cleofas-Sanchez, Garcia, Marques, and Senchez (2016), but corroborates, for instance, Wang, Ma, and Yang (2014), Liang, Lu, Tsai, and Shih (2016), and Cano et al. (2017). Our study supports the applicability of the expert systems by practitioners as in Heo and Yang (2014), Kim, Kang, and Kim (2015) and Xiao, Xiao, and Wang (2016).}
}

@article{sigrist,
title = {Machine learning for corporate default risk: Multi-period prediction, frailty correlation, loan portfolios, and tail probabilities},
journal = {European Journal of Operational Research},
volume = {305},
number = {3},
pages = {1390-1406},
year = {2023},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2022.06.035},
url = {https://www.sciencedirect.com/science/article/pii/S0377221722005100},
author = {Fabio Sigrist and Nicola Leuenberger},
keywords = {Risk analysis, OR in banking, Bankruptcy modeling, Non-linear model, Mixed effects model},
abstract = {We model multi-period cumulative and forward corporate default probabilities using machine learning methods and introduce a novel hybrid econometric-machine learning model which combines tree-boosting with a latent frailty model. The latter allows for modeling correlation that is not accounted for by observable predictor variables. We find that machine learning methods have higher prediction accuracy compared to linear models with the differences being larger for longer prediction horizons. The likely reason for this is the presence of stronger interaction effects for longer prediction horizons compared to short horizons. Among all methods, tree-boosting has the highest prediction accuracy. Further, the frailty component of the newly proposed “LaGaBoost frailty model” is overall large and exhibits strong variation over time. In contrast to prior research, we find that upper tail predictions of loan portfolio losses of frailty models are not consistently higher throughout time compared to models ignoring frailty correlation, but they show more temporal variation.}
}

@article{mai,
title = {Deep learning models for bankruptcy prediction using textual disclosures},
journal = {European Journal of Operational Research},
volume = {274},
number = {2},
pages = {743-758},
year = {2019},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2018.10.024},
url = {https://www.sciencedirect.com/science/article/pii/S0377221718308774},
author = {Feng Mai and Shaonan Tian and Chihoon Lee and Ling Ma},
keywords = {Decision support systems, Deep learning, Bankruptcy prediction, Machine learning, Textual data},
abstract = {This study introduces deep learning models for corporate bankruptcy forecasting using textual disclosures. Although textual data are common, it is rarely considered in the financial decision support models. Deep learning uses layers of neural networks to extract features from textual data for prediction. We construct a comprehensive bankruptcy database of 11,827 U.S. public companies and show that deep learning models yield superior prediction performance in forecasting bankruptcy using textual disclosures. When textual data are used in conjunction with traditional accounting-based ratio and market-based variables, deep learning models can further improve the prediction accuracy. We also investigate the effectiveness of two deep learning architectures. Interestingly, our empirical results show that simpler models such as averaging embedding are more effective than convolutional neural networks. Our results provide the first large-sample evidence for the predictive power of textual disclosures.}
}

@article{korangi,
title = {A transformer-based model for default prediction in mid-cap corporate markets},
journal = {European Journal of Operational Research},
volume = {308},
number = {1},
pages = {306-320},
year = {2023},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2022.10.032},
url = {https://www.sciencedirect.com/science/article/pii/S0377221722008207},
author = {Kamesh Korangi and Christophe Mues and Cristián Bravo},
keywords = {OR in banking, Mid-cap credit risk, Default prediction, Deep learning, Transformers},
abstract = {In this paper, we study mid-cap companies, i.e. publicly traded companies with less than US$10 billion in market capitalisation. Using a large dataset of US mid-cap companies observed over 30 years, we look to predict the default probability term structure over the short to medium term and understand which data sources (i.e. fundamental, market or pricing data) contribute most to the default risk. Whereas existing methods typically require that data from different time periods are first aggregated and turned into cross-sectional features, we frame the problem as a multi-label panel data classification problem. To tackle it, we then employ transformer models, a state-of-the-art deep learning model emanating from the natural language processing domain. To make this approach suitable to the given credit risk setting, we use a loss function for multi-label classification, to deal with the term structure, and propose a multi-channel architecture with differential training that allows the model to use all input data efficiently. Our results show that the proposed deep learning architecture produces superior performance, resulting in a sizeable improvement in AUC (Area Under the receiver operating characteristic Curve) over traditional models. In order to interpret the model, we also demonstrate how to produce an importance ranking for the different data sources and their temporal relationships, using a Shapley approach for feature groups.}
}

@misc{tian,
author = {Tian, Jiahao and Zhao, Jinman and Wang, Zhenkai and Ding, Zhicheng},
year = {2024},
month = {08},
title = {MMREC: LLM Based Multi-Modal Recommender System},
doi = {10.48550/arXiv.2408.04211}
}

@misc{cobbe,
author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
year = {2021},
month = {10},
title = {Training Verifiers to Solve Math Word Problems},
doi = {10.48550/arXiv.2110.14168}
}

@article{ahn,
  title={Large language models for mathematical reasoning: Progresses and challenges},
  author={Ahn, Janice and Verma, Rishu and Lou, Renze and Liu, Di and Zhang, Rui and Yin, Wenpeng},
  journal={arXiv preprint arXiv:2402.00157},
  year={2024}
}

@inproceedings{horie,
 author = {Horie, Masanobu and Mitsume, Naoto},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {23218--23229},
 publisher = {Curran Associates, Inc.},
 title = {Physics-Embedded Neural Networks: Graph Neural PDE Solvers with Mixed Boundary Conditions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/93476ae409ae3246e22a9d4b931f84ed-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@misc{zhang,
Author = {Xuan Zhang and Jacob Helwig and Yuchao Lin and Yaochen Xie and Cong Fu and Stephan Wojtowytsch and Shuiwang Ji},
Title = {SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations},
Year = {2024},
month = {03},
url = {https://arxiv.org/abs/2403.19507},
}

@misc{perozzi,
Author = {Bryan Perozzi and Bahare Fatemi and Dustin Zelle and Anton Tsitsulin and Mehran Kazemi and Rami Al-Rfou and Jonathan Halcrow},
Title = {Let Your Graph Do the Talking: Encoding Structured Data for LLMs},
Year = {2024},
Eprint = {arXiv:2402.05862},
}